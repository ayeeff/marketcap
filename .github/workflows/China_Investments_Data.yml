name: Scrape China Investments Data

on:
  schedule:
    # Run every month on the 1st at 00:00 UTC
    - cron: '0 0 1 * *'
  workflow_dispatch:  # Allows manual trigger from Actions tab

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Prevent indefinite hanging
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install selenium pandas PyGithub webdriver-manager
        
    - name: Set up Chrome
      uses: browser-actions/setup-chrome@latest
      with:
        chrome-version: stable
        
    - name: Run scraper
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        CI: true
      run: |
        timeout 20m python china_investment_tracker.py || exit 1
        
    - name: Upload debug files on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: debug-output-${{ github.run_number }}
        path: |
          error_screenshot.png
          page_source.html
        retention-days: 7
        
    - name: Upload CSV on success
      if: success()
      uses: actions/upload-artifact@v4
      with:
        name: china-investments-${{ github.run_number }}
        path: data/china_investments.csv
        retention-days: 30
